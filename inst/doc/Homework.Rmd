---
title: "Homeworks by 22030"
author: "22030"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homeworks by 22030}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r setup}
library(StatComp22030)
```

# A-22030-2022-09-09

## Question

Use knitr to produce at least 3 examples (texts, ﬁgures, tables).

## Answer

#### Example 1: Texts

I use the Boston dataset in the package {MASS}, which shows housing values in suburbs of Boston. 

```{r}
library(MASS) 
attach(Boston) # fix the Boston dataset
names(Boston) # column names of the data frame
```

Then, I run linear regression using medv (median value of owner-occupied homes in \$1000s) as the dependent variable and age (proportion of owner-occupied units built prior to 1940) as the independent variable.
```{r}
lm.Boston=lm(medv~age,data=Boston)
summary(lm.Boston)
```
Since $p$-value$<0.05$, the coefficients are significantly different from 0 at the 95% confidence level. Besides, the $R^2$ is `r summary(lm.Boston)$r.squared` and the regression coefficients are `r coef(lm.Boston)`.

#### Example 2: Figures

I plot the scatter plot and regression line in the figure below.
```{r}
plot(age,medv) # the scatter plot
abline(lm.Boston,lwd=1.5) # the regression line
title('Boston housing values')
```

#### Example 3: Tables

I plot a table of the first 5 rows of the Boston dataset.
```{r}
knitr::kable(head(Boston))
```


# A-22030-2022-09-15

## Question
3.3 The Pareto$(a, b)$ distribution has cdf
$$
F(x)=1-\left(\frac{b}{x}\right)^a, \quad x \geq b>0, a>0.
$$
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto$(2, 2)$ distribution. Graph the density histogram of the sample with the Pareto$(2, 2)$ density superimposed for comparison.

## Answer
Let $F(x)=U$, after calculation, we have
$$
x=\frac{b}{(1-U)^{1/a}},
$$
i.e., the probability inverse transformation
$$
F^{-1}(U)=\frac{b}{(1-U)^{1/a}}.
$$

Then, I use the inverse transform method to simulate a random sample from the Pareto$(2,2)$ distribution and graph the density histogram of the sample with the Pareto$(2, 2)$ density superimposed for comparison.
```{r}
set.seed(50)
a <- 2; b <- 2 # Pareto(2,2)
n <- 1000
u <- runif(n)
x <- b/(1-u)^{1/a}
hist(x, prob = TRUE, main = expression(f(x)==8/x^3 )) # the density f(x)=8/x^3
y <- seq(2,50,0.01)
lines(y, 8/y^3)
```


## Question
3.7 Write a function to generate a random sample of size n from the Beta$(a, b)$ distribution by the acceptance-rejection method. Generate a random sample of size 1000 from the Beta$(3,2)$ distribution. Graph the histogram of the sample with the theoretical Beta$(3,2)$ density superimposed.

## Answer
First, I write a function rBeta to generate a random sample of size n from the Beta$(a, b)$ distribution by the acceptance-rejection method. We let $g(x) = 1, 0 < x < 1$ and $c=1/B(a,b)$, where $B(a,b)$ is the Beta function and satisfies that $B(a,b)=\Gamma(\alpha) \Gamma(\beta)/ \Gamma(\alpha+\beta)$.
```{r}
rBeta<-function(n,a,b){
  j<-k<-0; y <- numeric(n)
  while (k < n) {
  u <- runif(1)
  j <- j + 1
  x <- runif(1) # random variate from g(.)
  if (x^(a-1) * (1-x)^(b-1) > u) {
    # we accept x
    k <- k + 1
    y[k] <- x
  }
  }
  return(y)
}
```

Then, I generate a random sample of size 1000 from the Beta$(3,2)$ distribution and graph the histogram of the sample with the theoretical Beta$(3,2)$ density superimposed.
```{r}
set.seed(35)
x <- rBeta(1000,3,2)
hist(x, prob = TRUE, main = expression(f(x)==x^2*(1-x)/B(3,2) )) # the density f(x)=x^2*(1-x)/B(3,2)
y <- seq(0,1,0.01)
lines(y, y^2*(1-y)/beta(3,2))
```


## Question
3.12 Simulate a continuous Exponential-Gamma mixture. Suppose that the rate parameter $\Lambda$ has $\operatorname{Gamma}(r, \beta)$ distribution and $Y$ has $\operatorname{Exp}(\Lambda)$ distribution. That is, $(Y \mid \Lambda=\lambda) \sim f_Y(y \mid \lambda)=\lambda e^{-\lambda y}$. Generate 1000 random observations from this mixture with $r=4$ and $\beta=2$.

## Answer
I generate 1000 random observations from a continuous Exponential-Gamma mixture with $r=4$ and $\beta=2$.
```{r}
set.seed(233)
n <- 1000; r <- 4; beta <- 2
lambda <- rgamma(n, r, beta)
x <- rexp(n, lambda)
```


## Question
3.13 It can be shown that the mixture in Exercise $3.12$ has a Pareto distribution with cdf
\begin{align*}
F(y)=1-\left(\frac{\beta}{\beta+y}\right)^r, \quad y \geq 0 .
\end{align*}
(This is an alternative parameterization of the Pareto cdf given in Exercise 3.3.) Generate 1000 random observations from the mixture with $r=4$ and $\beta=2$. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.

## Answer
I generate 1000 random observations from the mixture with $r=4$ and $\beta=2$, and graph the density histogram of the sample with the Pareto density curve superimposed.
```{r}
set.seed(25)
n <- 1000; r <- 4; beta <- 2
lambda <- rgamma(n, r, beta)
x <- rexp(n, lambda)
hist(x, prob = TRUE, main = expression(f(x)==64/(2+x)^5 ) ) #the density f(x)=64/(2+x)^5
y<-seq(0,10,0.01)
lines(y,64/(2+y)^5)
```

As is shown in the figure, the empirical and theoretical (Pareto) distributions is similar.


# A-22030-2022-09-23

## Question
**1:**

- For $n=10^4, 2 \times 10^4, 4 \times 10^4, 6 \times 10^4, 8 \times 10^4$, apply the fast sorting algorithm to randomly permuted numbers of $1, \ldots, n$.

- Calculate computation time averaged over 100 simulations, denoted by $a_n$.

- Regress $a_n$ on $t_n:=n \log (n)$, and graphically show the results (scatter plot and regression line).

## Answer
```{r}
# the fast sorting algorithm
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))}
}
```

Fistly, I apply the fast sorting algorithm to randomly permuted numbers of $1, \ldots, n$, and calculate computation time averaged over 100 simulations, denoted by $a$.
```{r}
# since we have 100 replications, I don't use random number seeds.
n.set<-c(1e4,2e4,4e4,6e4,8e4)
time <-matrix(0,100,5) # initialize time
for (r in 1:100) { # the rth replication
  for (i in 1:5) {
    n<-n.set[i]
    time[r,i]=system.time(quick_sort(sample(1:n)))[1]
  }
  a<-colMeans(time) # a is the computation time averaged over 100 simulations
}
a
```

Then, I regress $a_n$ on $t_n:=n \log (n)$, and graphically show the results (scatter plot and regression line).
```{r}
t<-n.set*log(n.set)
lm.fit<-lm(a~t)
plot(t,a,xlab = expression(n*log(n)),ylab = 'Computation time') # plot the scatter plot
abline(lm.fit,lwd=1.5) # plot the regression line
```

## Question
**Exercises 5.6:**

In Example $5.7$ the control variate approach was illustrated for Monte Carlo integration of
\begin{align*}
\theta=\int_0^1 e^x d x .
\end{align*}
Now consider the antithetic variate approach. Compute $\operatorname{Cov}\left(e^U, e^{1-U}\right)$ and $\operatorname{Var}\left(e^U+e^{1-U}\right)$, where $U \sim \operatorname{Uniform}(0,1)$. What is the percent reduction in variance of $\hat{\theta}$ that can be achieved using antithetic variates (compared with simple MC)?

## Answer
$$
\operatorname{Cov}\left(e^U, e^{1-U}\right)
=\operatorname{E}\left[e^Ue^{1-U}\right]-\operatorname{E}\left[e^U\right]\operatorname{E}\left[e^{1-U}\right]
=e-(e-1)^2
\approx -0.2342106.
$$
Therefore, $e^U$ and $e^{1-U}$ are negatively correlated.
$$
\operatorname{Var}\left(e^U\right)
=\operatorname{E}\left[e^{2U}\right]-\operatorname{E}\left[e^U\right]^2=(e^2-1)/2-(e-1)^2,
$$
Similarily, we have
$$
\operatorname{Var}\left(e^{1-U}\right)=(e^2-1)/2-(e-1)^2.
$$
Therefore,
$$
\begin{aligned}
&\operatorname{Var}\left(e^U+e^{1-U}\right) \\
=&\operatorname{Var}\left(e^U\right)+\operatorname{Var}\left(e^{1-U}\right)+2\operatorname{Cov}\left(e^U, e^{1-U}\right) \\
=&(e^2-1)-2(e-1)^2+2[e-(e-1)^2]\\
=&e^2+2e-1-4(e-1)^2 \\
\approx & 0.01564999.
\end{aligned}
$$

Assume that $U_1,\ldots,U_{m/2},U_{m/2+1},\ldots,U_{m}$ are iid,
$U_j \sim \operatorname{Uniform}(0,1)$.
The simple estimator is
$$
\hat{\theta}_1=\frac{1}{m}\sum_{j=1}^{m}e^{U_j},
$$
The antithetic variable estimator is
$$
\hat{\theta}_2=\frac{1}{m}\sum_{j=1}^{m/2}\left( e^{U_j}+e^{1-U_j} \right).
$$
The percent reduction in variance of $\hat{\theta}$ that can be achieved using antithetic variates (compared with simple MC) is
$$
\begin{aligned}
&\frac{\operatorname{Var}\left(\hat{\theta}_1\right)-\operatorname{Var}\left(\hat{\theta}_2\right)}{\operatorname{Var}\left(\hat{\theta}_1\right)} \\
=&\frac{\operatorname{Var}\left(e^{U_1}\right)-\operatorname{Var}\left(e^{U_1}+e^{1-U_1}\right)/2}{\operatorname{Var}\left(e^{U_1}\right)} \\
=&\frac{(e^2-1)/2-(e-1)^2-[e^2+2e-1-4(e-1)^2]/2}{(e^2-1)/2-(e-1)^2} \\
=&2(e^2-3e+1)/(-e^2+4e-3) \\
\approx &0.9676701.
\end{aligned}
$$

## Question
**Exercises 5.7:**

Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.

## Answer
Before answering the question, I write down a flowchat.

1. Specify $m$, the sample size, with default $m=1000$.

2. Generate random numbers $U_1,\ldots, U_{m/2}$ from the uniform distribution U(0,1).

3. If antithetic = TRUE, we use the antithetic variate approach and define $U_{m/2+1}=1-U_1, \ldots, U_{m}=1-U_{m/2}$.
If antithetic = FALSE, we use the simple Monte Carlo method and generate new random numbers $U_{m/2+1},\ldots, U_{m}$ from the uniform distribution U(0,1).

4. Calculate
$$
\hat{\theta}=\frac{1}{m}\sum_{j=1}^{m}e^{U_j}.
$$
5. Output result $\hat{\theta}$.


According to the flowchart, now I can define a function to estimate theta by the antithetic variate approach and by the simple Monte Carlo method.
```{r}
# m is the sample size
# if antithetic = TRUE, we use the antithetic variate approach
# if antithetic = FALSE, we use the simple Monte Carlo method
MC <- function(m = 10000, antithetic = FALSE) {
  u <- runif(m/2)
  if (antithetic) v <- 1-u else v <- runif(m/2)
  u <- c(u, v)
  g <- exp(u)
  theta <- mean(g)
  return(theta)
}
```

Then, I use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method.
```{r}
set.seed(999)
MC.1<-MC(antithetic = FALSE) # the simple Monte Carlo estimator
MC.2<-MC(antithetic = TRUE) # the antithetic variable estimator
c(MC.1,MC.2)
```
We can see that the result is similar.

Finally, I compute an empirical estimate of the percent reduction in variance using the antithetic variate after 1000 replications.
```{r}
R<-1000
theta.1 <- theta.2 <- numeric(R) # initialize
for (i in 1:R) {
  theta.1[i] <- MC(m = 1000, antithetic = FALSE) # the simple estimator
  theta.2[i] <- MC(m = 1000, antithetic = TRUE) # the antithetic variable estimator
}
# an empirical estimate of the percent reduction in variance
(var(theta.1)-var(theta.2))/var(theta.1)
```
We can see that the empirical estimate of the percent reduction is close to the theoretical value from Exercise 5.6.

# A-22030-2022-09-30

## Question{#question}

1. Exercises 5.13 (Page 151, Statistical Computing with R). [Jump to the Answer](#question1ans)

2. Exercises 5.15 (Page 151, Statistical Computing with R). [Jump to the Answer](#question2ans)

There is something wrong with the subintervals in Exercise 5.15 (Example 5.13). You may modify it without losing the original intent.



## Answer

### Exercise 5.13{#question1ans}
**Problem.**
Find two importance functions $f_1$ and $f_2$ that are supported on $(1, \infty)$ and are 'close' to
$$
g(x)=\frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2}, \quad x>1 .
$$
Which of your two importance functions should produce the smaller variance in estimating
$$
\int_1^{\infty} \frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2} d x
$$
by importance sampling? Explain.

**Solution.**

**1. Find two importance functions $f_1$ and $f_2$ **

Since we need to find two importance functions $f_1$ and $f_2$ that are supported on $(1, \infty)$, we may consider a truncated normal distribution or a transformd $\chi^2$ distribution.

Define
$$
f_1=2 \frac{1}{\sqrt{2\pi}} e^{-(x-1)^2 / 2}, \quad x>1,
$$
which is a truncated N(1,1) distribution.

Define
$$
f_2=\frac{1}{2^{3/2}\Gamma(3/2)}(x-1)^{1/2} e^{-(x-1)/2}, \quad x>1,
$$
which is a transformd $\chi^2(3)$ distribution.
Both of $f_1$ and $f_2$ are supported on $(1, \infty)$.

Then, we display the graph of $g(x), f_1, f_2$.
```{r}
x<-seq(1, 10, 0.01)
g<-x^2*exp(-x^2/2)/sqrt(2 * pi)
plot(x, g, type = 'l', ylab = "", ylim = c(0,1), col=1, lwd=1.5, main = "function graph")
f_1<-2*dnorm(x,1)
lines(x, f_1, lty=2, col=2, lwd=1.5)
f_2<-dchisq(x-1,df=3)
lines(x, f_2, lty=3, col=3, lwd=1.5)
legend("topright",  legend = c("g(x)", expression(f[1](x)),expression(f[2](x))),
       inset = 0.02, lty=1:3, col=1:3)
```

According to the graph, I find that $f_1$ and $f_2$ are 'close' to $g(x)$.

**2. Compare two importance functions**

Now I draw a graph to compare the ratios $g(x) / f(x)$.
```{r}
plot(x, g/f_1, type = 'l',lty=2, ylab = "", ylim = c(0,6.5),
     col=2, lwd=1.5, main = expression(g(x) / f(x)))
lines(x, g/f_2, lty=3, col=3, lwd=1.5)
legend("topright",  legend = c(expression(g(x)/f[1](x)),expression(g(x)/f[2](x))),
       inset = 0.02, lty=2:3, col=2:3)
```

According to the graph, I find that $g(x) / f_1(x)$ is more stable and closer to a constant function, which means that $f_1$ should produce the smaller variance in estimating
$$
\int_1^{\infty} \frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2} d x
$$
by importance sampling.

Finally, I conduct the following simulations:
```{r}
set.seed(999)
m<-1e6
# using f1
x<- abs(rnorm(m))+1
f<-2*dnorm(x, 1)
g<-x^2*exp(-x^2/2)/sqrt(2 * pi)
theta_1<-mean(g/f)
var_1<-var(g/f)
# using f2
x<- rchisq(m,df=3)+1
f<-dchisq(x-1,df=3)
g<-x^2*exp(-x^2/2)/sqrt(2 * pi)
theta_2<-mean(g/f)
var_2<-var(g/f)
result<-rbind(estimate=c(theta_1,theta_2), variance=c(var_1,var_2))
colnames(result) <- c('f1','f2')
library(knitr)
knitr::kable(round(result,4))
```
According to the simulation result, I find that $f_1$ actually produce the smaller variance.





### Exercise 5.15{#question2ans}
**Problem.**
Obtain the stratiﬁed importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

**Solution.**

**1. Obtain the stratiﬁed importance sampling estimate in Example 5.13**

Specifically，we need to
obtain the stratified importance sampling estimate of
$$
\theta=\int_0^1 \frac{e^{-x}}{1+x^2}
$$
with importance function
$$
f(x)=\frac{e^{-x}}{1-e^{-1}}, \quad 0<x<1
$$
on five subintervals, $(a_{j-1},a_j), j=1, \ldots, 5$. On the $j^{t h}$ subinterval variables are generated from the density
$$
f_j(x)=f_{x \mid I_j}(x)=\frac{5 e^{-x}}{1-e^{-1}}, \quad a_{j-1}<x<a_{j}.
$$

First, I derive $a_j, j=0,1, \ldots, 5$. Since $a_0=0, a_j=F^{-1}(j / 5), j=1, \ldots, 4$, and $a_5=1$, where $F$ is the cdf corresponding to $f$, we have
$$
a_j=\log(\frac{5}{5-j+je^{-1}}), \quad j=0, \ldots, 5.
$$
We can generate samples from $f_j$ on $(a_{j-1},a_j)$ by the inverse transform method.  Specifically, I generate sample $u$ from $U((j-1)/5,j/5)$ and then let
$$
x=-\log(1-u(1-e^{-1})), \quad (j-1)/5<u<j/5
$$
for $j=1, \ldots, 5$.

Now, I obtain the stratified importance sampling estimate of $\theta$.
```{r}
set.seed(233)
M<-1e6
k<-5
var<-theta<-numeric(k)
g <- function(x) exp(-x)/(1 + x^2)
f <- function(x) exp(-x)/(1 - exp(-1))
for (j in 1:k) {
  # generate samples from fj by the inverse transform method
  u<-runif(M/k, (j-1)/5,j/5)
  x<--log(1-u*(1-exp(-1)))
  theta[j]<-mean(g(x)/f(x))
  var[j]<-var(g(x)/f(x))
}

# the stratified importance sampling estimate of theta
round(mean(theta),4)
# the standard deviation of theta multiplied by sqrt(M)
round(sqrt(mean(var)),4)
```






**2. Compare it with the result of Example 5.10**

The result of Example 5.10 is shown below.
```{r,echo=FALSE}
  m <- 1e6
  est <- sd <- numeric(5)
  g <- function(x) {
  exp(-x - log(1+x^2)) * (x > 0) * (x < 1)
  }
  x <- runif(m) #using f0
  fg <- g(x)
  est[1] <- mean(fg)
  sd[1] <- sd(fg)
  x <- rexp(m, 1) #using f1
  fg <- g(x) / exp(-x)
  est[2] <- mean(fg)
  sd[2] <- sd(fg)
  x <- rcauchy(m) #using f2
  i <- c(which(x > 1), which(x < 0))
  x[i] <- 2 #to catch overflow errors in g(x)
  fg <- g(x) / dcauchy(x)
  est[3] <- mean(fg)
  sd[3] <- sd(fg)
  u <- runif(m) #f3, inverse transform method
  x <- - log(1 - u * (1 - exp(-1)))
  fg <- g(x) / (exp(-x) / (1 - exp(-1)))
  est[4] <- mean(fg)
  sd[4] <- sd(fg)
  u <- runif(m) #f4, inverse transform method
  x <- tan(pi * u / 4)
  fg <- g(x) / (4 / ((1 + x^2) * pi))
  est[5] <- mean(fg)
  sd[5] <- sd(fg)
  res <- rbind(est=round(est,4), sd=round(sd,4))
  colnames(res) <- paste0('f',0:4)
  knitr::kable(res)
```
Comparing them with the stratiﬁed importance sampling estimate, we can find that the estimate of $\theta$ is similar, but the standard deviation of the stratiﬁed importance sampling estimate is much smaller than others.




--------

<center><font color=Red size=5>**This is the end of my assignment.**</font></center>

--------


# A-22030-2022-10-09

## Question{#question}

1. Exercises 6.4 (Page 180, Statistical Computing with R). [Jump to the Answer](#question3ans)

2. Exercises 6.8 (Page 181, Statistical Computing with R). [Jump to the Answer](#question4ans)

3. Discussion (homework)

- If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments:
say, 0.651 for one method and 0.676 for another method. Can we say the powers are diﬀerent at 0.05 level?

- What is the corresponding hypothesis test problem?

- Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

- Please provide the least necessary information for hypothesis testing. [Jump to the Answer](#question5ans)

## Answer

### Exercise 6.4{#question3ans}
**Problem.**
Suppose that $X_1, \ldots, X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a $95 \%$ confidence interval for the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

**Solution.**
Define $Y_i=\log(X_i),i=1,\ldots, n$, then $Y_i$ is normal. We can estimate $\mu$ using the sample mean of $\bar{Y}$ and a $1-p$  confidence interval of $\log(\mu)$ is given by
$$
(\bar{Y}+\frac{\hat{\sigma}}{\sqrt{n}}z_{p/2}, \bar{Y}+\frac{\hat{\sigma}}{\sqrt{n}}z_{1-p/2} ),
$$
where $\hat{\sigma}$ is the sample standard deviation of $Y_i,i=1,\ldots, n$.

Now I define the following function CI to find a confidence interval of $\log(\mu)=0$, i.e. $\mu=1$.
```{r}
rm(list=ls()) # clear up the memory
# define a function CI to find a confidence interval of logmu=0, i.e. mu=1
CI<-function(n,aplha){
  X<-rlnorm(n) # we can use rlnorm to generate the data, which is already a function
  Y<-log(X)
  se<-sd(Y)/sqrt(n)
  mean(Y) + se * qnorm(c(alpha/2, 1-alpha/2))
}
```
Now, we can construct a $95 \%$ confidence interval for the parameter $\log(\mu)=0$.
```{r}
n<-100
alpha<-0.05
set.seed(3278)
CI(n,alpha)
```
Finally, I use a Monte Carlo method to obtain an empirical estimate of the confidence level.
```{r}
# define a function MC to use a Monte Carlo method to obtain an empirical estimate of the confidence level
MC<-function(n,alpha,M){
  LU<-matrix(0,M,2)
  for (i in 1:M) {
    LU[i,]<-CI(n,alpha)
    }
  mean(LU[,1] < 0 & LU[,2] > 0) # an empirical estimate of the confidence level
}
# obtain an empirical estimate of the confidence with M=1e4
set.seed(22030)
M<-1e4
MC(n,alpha,M)
```
According to the result, we can see that the empirical estimate of the confidence level is close to the real confidence level 0.95.




### Exercise 6.8{#question4ans}
**Problem.**
Refer to Example 6.16. Repeat the simulation, but also compute the $F$ test of equal variance, at significance level $\hat{p} \doteq 0.055$. Compare the power of the Count Five test and $F$ test for small, medium, and large sample sizes. (Recall that the $F$ test is not applicable for non-normal distributions.)

**Solution.**
First, I define a function count5test to conduct count five test.
```{r}
rm(list=ls()) # clear up the memory
# define a function count5test to conduct count five test
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5)) }
```
Second, I define a function to generate samples under $H_1$ with a sample size of $n$ and conduct Count Five test and F test of equal variance.
```{r}
# generate samples under H1 to estimate power
sigma1 <- 1; sigma2 <- 1.5
# define a function to conduct of the Count Five test and F test with a sample size of n
test<-function(n){
  # we can use rnorm to generate the data, which is already a function
  x <- rnorm(n, 0, sigma1)
  y <- rnorm(n, 0, sigma2)
  C5t <- count5test(x, y) # count five test
  Fp <- var.test(x, y)$p.value
  Ftest <- as.integer(Fp < 0.055) # F test
  c(C5t, Ftest)
}
```
Third, I define a function to use Monte Carlo methods to estimate the power of the Count Five test and F test with a sample size of $n$  after $M$ replications.
```{r}
# define a function to estimate the power of the Count Five test and F test
power<-function(n,M){
  result<-matrix(0,M,2)
  for (i in 1:M) {
    result[i,]<-test(n)
  }
  colMeans(result)
}
```

Finally, I compute the power of the Count Five test and F test for $n=10,20,50,100,200,500,1000$.
```{r}
M<-1e4
nset<-c(10,20,50,100,200,500,1000)
set.seed(1643)
result<-matrix(0,length(nset),3)
colnames(result)<-c('n','count5test','Ftest')
for (i in 1:length(nset)) {
  n<-nset[i]
  result[i,]<-c(n,power(n,M))
}
result
```
The simulation results suggest that the F test for equal variance is more powerful in this cases. Moreover, the larger $n$ is, the more powerful the test is.




### Discussion{#question5ans}
**Problem.**

- If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments:
say, 0.651 for one method and 0.676 for another method. Can we say the powers are diﬀerent at 0.05 level?

- What is the corresponding hypothesis test problem?

- Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

- Please provide the least necessary information for hypothesis testing.

**Solution.**
<!-- **Method 1: Z-test** -->
According to deﬁnition of power, the MC estimator of power after $m$replications  estimator is
$$
\frac{1}{m} \sum_{j=1}^m I\left(p_j \leq \alpha\right)
$$
when $H_a$ holds. Let $X=\sum_{j=1}^m I\left(p_j \leq \alpha\right)$, it follows $B(m,p)$ distribution, where $p$ is the power.

Therefore, the corresponding hypothesis test problem is equivalent to test $H_0: p_1=p_2$ against $H_1: p_1 \neq p_2$ with observations $X_1 \sim B(m,p_1)$ and $X_2 \sim B(m,p_1)$.

<!-- There are many tests we can use for this hypothesis test problem, such as Clopper-Pearson method, Wald method, Wilson method and variance stable transformation(VST). I use Wald method so that we can use Z-test. -->

Write $\hat{p}_1=X_1 / m$ and $\hat{p}_2=X_2 / m$, then
$$
\sqrt{m} \frac{(\hat{p}_1-\hat{p}_2)-(p_1-p_2)}
{\sqrt{p_1(1-p_1)+p_2(1-p_2)}}
\rightarrow N(0,1).
$$
By strong law of large number, we have $\hat{p}_1 \rightarrow p_1, a.s.$ and $\hat{p}_2 \rightarrow p_2, a.s.$.Then, by Slutsky theorem, we obtain that
$$
 \sqrt{m} \frac{(\hat{p}_1-\hat{p}_2)-(p_1-p_2)}
{\sqrt{\hat{p}_1(1-\hat{p}_1)+\hat{p}_1(1-\hat{p}_1)}}
\rightarrow N(0,1).
$$
Define
$$
T \equiv \sqrt{m} \frac{\hat{p}_1-\hat{p}_2}
{\sqrt{\hat{p}_1(1-\hat{p}_1)+\hat{p}_1(1-\hat{p}_1)}} ,
$$
when $H_0$ holds, $T \rightarrow N(0,1)$. Therefore, we can use Z-test.
At significance level 0.05, when $H_0$ holds, we reject $H_0$ if $|T|>z_{0.975}$ and otherwise accept it.

Now we can conduct the following hypothesis test:
```{r}
rm(list=ls()) # clear up the memory
# define a function to conduct Z-test
Ztest<-function(m,p1,p2){
  Ts<-sqrt(m)*(p1-p2)/sqrt(p1*(1-p1)+p2*(1-p2))
  abs(Ts)>qnorm(0.975) # if TRUE, we rejuct H0
}
Ztest(1e4,0.651,0.676)
```
Therefore, when powers are 0.651 for one method and 0.676 for another method with 10,000 experiments, we reject $H_0$ and say that the powers are diﬀerent at 0.05 level.




--------

<center><font color=Red size=5>**This is the end of my assignment.**</font></center>

--------

# A-22030-2022-10-14

## Question{#question}

1. Exercises 7.4 (Page 212, Statistical Computing with R). [Jump to the Answer](#question6ans)

2. Exercises 7.5 (Page 212, Statistical Computing with R). [Jump to the Answer](#question7ans)

3. Exercises 7.A (Page 213, Statistical Computing with R). [Jump to the Answer](#question8ans)


## Answer

### Exercise 7.4{#question6ans}
**Problem.**
Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of airconditioning equipment [63, Example 1.1]:
\begin{align*}
3,5,7,18,43,85,91,98,100,130,230,487 .
\end{align*}
Assume that the times between failures follow an exponential model $\operatorname{Exp}(\lambda)$. Obtain the MLE of the hazard rate $\lambda$ and use bootstrap to estimate the bias and standard error of the estimate.

**Solution.**

Since the times between failures follow an exponential model $\operatorname{Exp}(\lambda)$, **the MLE of $\lambda$ is $1/\bar{X}$**, where $\bar{X}$ is the sample mean.

Then, I use bootstrap to estimate the bias and standard error of the estimate.
```{r}
library(boot)
set.seed(126)
X <- aircondit[1]
MLE <- function(X, i) 1/mean(as.matrix(X[i, ]))
# use bootstrap to estimate the bias and standard error of the estimate
boot(X, statistic=MLE, R=1e4)
rm(list=ls())
```




### Exercise 7.5{#question7ans}
**Problem.**
Refer to Exercise 7.4. Compute $95 \%$ bootstrap confidence intervals for the mean time between failures $1 / \lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

**Solution.**

First, I use the function boot.ci to compute bootstrap confidence intervals.
```{r}
set.seed(701)
X <- aircondit[1]
theta <- function(X, i) mean(as.matrix(X[i, ]))
(boot<-boot(X, statistic=theta, R=1e4))
# compute 95% bootstrap confidence intervals
(ci <- boot.ci(boot,type=c("norm","basic","perc","bca")))
```
According to the result, the confidence intervals of different methods are quite different. We draw a histgram first.
```{r}
hist(boot$t,main='Histgram of bootstrap statistic')
abline(v=mean(as.matrix(X)),col='red',lwd=2)
rm(list=ls())
```
From the histogram, we can see that the distribution of the bootstrap statistic is skewed. Since the sample size is too small, the mean time is not approximately normal, which means that the standard normal confidence interval is not suitable. The basic confidence interval is not suitable as well. The percentile only rely on percentile, so it is not so accurate. The BCa interval corrects the bias and adjusts the skewness, so it is much better.




### Exercise 7.A{#question8ans}
**Problem.**
Conduct a Monte Carlo study to estimate the coverage probabilities of the standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval. Sample from a normal population and check the empirical coverage rates for the sample mean. Find the proportion of times that the confidence intervals miss on the left, and the proportion of times that the confidence intervals miss on the right.

**Solution.**

I consider data generated from a standard norm distribution. Now I conduct a Monte Carlo study and find the empirical coverage rates for the sample mean, the proportion of times that the confidence intervals miss on the left, and the proportion of times that the confidence intervals miss on the right.
```{r}
set.seed(23387)
mu<-0;sigma<-1;m<-1e4;n<-100
mean.sample <- function(X,i) mean(X[i])
ci.norm<-ci.basic<-ci.perc<-matrix(0,m,2)
# conduct a Monte Carlo simulation
for(i in 1:m){
  X<-rnorm(n,mu,sigma)
  boot<-boot(data=X, statistic=mean.sample, R = 100)
  ci<-boot.ci(boot,type=c("norm","basic","perc"))
  ci.norm[i,]<-ci$norm[2:3]
  ci.basic[i,]<-ci$basic[4:5]
  ci.perc[i,]<-ci$percent[4:5]
}
# the empirical coverage rates for the sample mean
(result1<-c("norm"=mean(ci.norm[,1]<=mu & ci.norm[,2]>=mu),
            "basic"=mean(ci.basic[,1]<=mu & ci.basic[,2]>=mu),
            "perc"=mean(ci.perc[,1]<=mu & ci.perc[,2]>=mu)))
# the proportion of times that the confidence intervals miss on the left
(result2<-c("norm"=mean(ci.norm[,2]< mu),
            "basic"=mean(ci.basic[,2]< mu),
            "perc"=mean(ci.perc[,2]< mu)))
# the proportion of times that the confidence intervals miss on the right
(result3<-c("norm"=mean(ci.norm[,1]> mu),
            "basic"=mean(ci.basic[,1]> mu),
            "perc"=mean(ci.perc[,1]> mu)))
rm(list=ls())
```






--------

<center><font color=Red size=5>**This is the end of my assignment.**</font></center>

--------

# A-22030-2022-10-21

## Question{#question}

1. Exercises 7.8 (Page 213, Statistical Computing with R). [Jump to the Answer](#question9ans)

2. Exercises 7.11 (Page 213, Statistical Computing with R). [Jump to the Answer](#question10ans)

3. Exercises 8.2 (Page 242, Statistical Computing with R). [Jump to the Answer](#question11ans)

## Answer

### Exercise 7.8{#question9ans}
**Problem.**
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

**Solution.**
Now I compute the jackknife estimates of bias and standard error of $\hat{\theta}$.
```{r}
library(bootstrap)
x <- as.matrix(scor)
n <- nrow(x)
lambda <- eigen(cov(x))$values
theta.hat <- max(lambda/sum(lambda)) # original estimate
theta.jack <- numeric(n)
for(i in 1:n){
  lambda.jack <- eigen(cov(x[-i, ]))$values
  theta.jack[i] <- max(lambda.jack/sum(lambda.jack)) # jackknife estimate
}
# the jackknife estimates of bias
bias.jack <- (n-1)*(mean(theta.jack) - theta.hat)
# the jackknife estimates of standard error
se.jack <- sqrt((n-1)*mean((theta.jack - mean(theta.jack))^2))
c(est=theta.hat, bias=bias.jack, se=se.jack)
```
The jackknife estimate of bias of is `r bias.jack` and the jackknife estimate of standard error is `r se.jack`.
```{r}
rm(list = ls())
```




### Exercise 7.11{#question10ans}
**Problem.**
In Example 7.18, leave-one-out ( $n$-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.


**Solution.**
Now I use leave-two-out cross validation to compare the four models:

1. Linear: $Y=\beta_0+\beta_1 X+\varepsilon$.

2. Quadratic: $Y=\beta_0+\beta_1 X+\beta_2 X^2+\varepsilon$.

3. Exponential: $\log (Y)=\log \left(\beta_0\right)+\beta_1 X+\varepsilon$.

4. Log-Log: $\log (Y)=\beta_0+\beta_1 \log (X)+\varepsilon$.

```{r}
library(DAAG); attach(ironslag)
n <- length(magnetic)
N <- n*(n-1)/2 # all possible combination
e1 <- e2 <- e3 <- e4 <- numeric(n)
h<-1
# leave-two-out cross validation
for (i in 1:(n-1)) {
  for (j in (i+1):n) {
    k<-c(i,j)
    y <- magnetic[-k]
    x <- chemical[-k]
    # Model 1: Linear
    J1 <- lm(y ~ x)
    yhat1 <- J1$coef[1] + J1$coef[2]*chemical[k]
    e1[h] <- sum((magnetic[k] - yhat1)^2)
    # Model 2：Quadratic
    J2 <- lm(y ~ x + I(x^2))
    yhat2 <- J2$coef[1] + J2$coef[2]*chemical[k] +J2$coef[3]*chemical[k]^2
    e2[h] <- sum((magnetic[k] - yhat2)^2)
    # Model 3: Exponential
    J3 <- lm(log(y) ~ x)
    logyhat3 <- J3$coef[1] + J3$coef[2]*chemical[k]
    yhat3 <- exp(logyhat3)
    e3[h] <- sum((magnetic[k] - yhat3)^2)
    # Model 4: Log-Log
    J4 <- lm(log(y) ~ log(x))
    logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
    yhat4 <- exp(logyhat4)
    e4[h] <- sum((magnetic[k] - yhat4)^2)
    h<-h+1
  }
}
# the average squared prediction error by leave-two-out cross validation
c(Linear=sum(e1), Quadratic=sum(e2), Exponential=sum(e3), LogLog=sum(e4))/(2*N)
```
As is shown above, the quadratic model is best according to the minimum average squared prediction error by leave-two-out cross validation.
```{r}
rm(list = ls())
```




### Exercise 8.2{#question11ans}
**Problem.**
Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved signiﬁcance level of the permutation test with the $p$-value reported by cor.test on the same samples.

**Solution.**
First, I define a function to implement the bivariate Spearman rank correlation test for independence as a permutation test, which returns the p-value.
```{r}
# define a function to conduct permutation test
spear.perm <- function(x, y, R=1e3){
  rho0<-cor.test(x, y, method = "spearman")$estimate # the original test estimate
  n<-length(y)
  reps <- numeric(R)
  for (i in 1:R) {
    k <- sample(1:n)
    reps[i] <- cor.test(x, y[k], method = "spearman")$estimate
    }
  p <- mean(c(rho0, reps) >= rho0) # p-value of the permutation test
  return(p) # return p-value
}
```

Then, I generate data from multi-normal distribution and conduct the permutation test.
```{r}
library(MASS)
# generate data from multi-normal distribution
mu <- c(0, 0); sigma <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
n<-30
set.seed(312)
xy <- mvrnorm(n, mu, sigma)
x<-xy[, 1]; y<-xy[, 2]
# the p-value reported by cor.test on the same samples
(p0<-cor.test(x, y, method = "spearman")$p.value)
# the achieved signiﬁcance level of the permutation test
(p.perm<- spear.perm(x,y))
# compare the two p-value
round(c(p0=p0,perm=p.perm),4)
```
Comparing the achieved signiﬁcance level of the permutation test with the $p$-value reported by cor.test on the same samples, we find that they are close and both signiﬁcant. We can both reject the null hypothesis that $x$ and $y$ are independent.

```{r}
rm(list = ls())
```



--------

<center><font color=Red size=5>**This is the end of my assignment.**</font></center>

--------

# A-22030-2022-10-28

## Question{#question}

1. Exercises 9.4 (Page 277, Statistical Computing with R). [Jump to the Answer](#question12ans)

2. Exercises 9.7 (Page 278, Statistical Computing with R). [Jump to the Answer](#question13ans)

For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R}<1.2$.


## Answer

### Exercise 9.4{#question12ans}
**Problem.**
Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

**Solution.**
The pdf of standard Laplace distribution is
$$
f(x)=\frac{1}{2} e^{-|x|}.
$$
Then we have
$$
\alpha\left(x_{i-1}, y\right)=\frac{f(y)}{f\left(x_{i-1}\right)}=\frac{e^{-|y|}}{e^{-\left|x_{i-1}\right|}}=e^{\left|x_{i-1}\right|-|y|}
$$
Now I define a function to implement a random walk Metropolis sampler for generating the standard Laplace distribution.
```{r}
# define a function to implement a random walk Metropolis sampler
rw.Laplace <- function(N, x0, sigma) {
  # N is the length of chain, x0 is the initial value
  # sigma is the standard deviation of the normal proposal distribution
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= exp(abs(x[i-1]) - abs(y)))
      x[i] <- y
    else {
      x[i] <- x[i-1]
      k <- k + 1
    }
  }
  return(list(x = x, k = k))
}
```
Then, I generate chains where different variances are used for the proposal distribution and compare them.
```{r}
N <- 10000 # length of chains
sigma <- c(0.5, 1, 2, 4) # the standard deviation of the normal proposal distribution
x0 <- 0 # initial values
set.seed(123)
rw1 <- rw.Laplace(N, x0, sigma[1])
rw2 <- rw.Laplace(N, x0, sigma[2])
rw3 <- rw.Laplace(N, x0, sigma[3])
rw4 <- rw.Laplace(N, x0, sigma[4])
par(mfrow=c(2,2))  #display 4 graphs together
rw <- cbind(rw1$x, rw2$x, rw3$x, rw4$x)
# for (j in 1:4) {
#   plot(rw[,j], type="l",xlab=bquote(sigma == .(round(sigma[j],3))),
#        ylab="X", ylim=range(rw[,j]))
#   }
par(mfrow=c(1,1)) #reset to default
```
As is shown above, each of the chains appear to have converged to the target Laplace distribution.

Now, I compute the acceptance rates of each chain.
```{r}
acceptance.rate <- data.frame(sigma=sigma, acceptance.rate=1-c(rw1$k, rw2$k, rw3$k, rw4$k)/N)
knitr::kable(acceptance.rate,align = 'c')
```
Finally, I use the Gelman-Rubin method to monitor convergence of the chain.
```{r}
# define a function to use the Gelman-Rubin method to monitor convergence of the chain
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance estimation
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within estimation
  v.hat <- W*(n-1)/n + (B/n)     #upper variance estimation
  r.hat <- v.hat / W             #estimation
  return(r.hat)
}

N <- 15000 # length of chains
k <- 4    # number of chains to generate
sigma <- 1 # the standard deviation of the normal proposal distribution
b <- 1000       # burn-in length
# choose overdispersed initial values
x0 <- c(-4, -2, 2, 4)
set.seed(25)
# generate the chains
X <- matrix(0, nrow=k, ncol=N)
for (i in 1:k){
  X[i, ] <- rw.Laplace(N, x0[i], sigma)$x
}

# compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi)){
  psi[i,] <- psi[i,] / (1:ncol(psi))
}
# plot psi for the four chains
for (i in 1:k){
  if(i==1){
    plot((b+1):N,psi[i, (b+1):N], ylim=c(-0.5,0.5),type="l",
         xlab='Index', ylab=bquote(phi))
    }else{
      lines(psi[i, (b+1):N], col=i)
    }
}
# plot the sequence of R-hat statistics
rhat <- rep(0, N)
for (j in (b+1):N){
  rhat[j] <- Gelman.Rubin(psi[,1:j])
}
plot(rhat[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```

As is shown above, the chain converges to the target distribution according to $\hat{R}<1.2$ when $n$ is approximately 3200 (1000+2200).
```{r}
rm(list = ls())
```




### Exercise 9.7{#question13ans}
**Problem.**
Implement a Gibbs sampler to generate a bivariate normal chain $\left(X_t, Y_t\right)$ with zero means, unit standard deviations, and correlation 0.9. Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model $Y=\beta_0+\beta_1 X$ to the sample and check the residuals of the model for normality and constant variance.

**Solution.**
First, I define a function to use Gibbs sampler to generate  a bivariate normal chain $\left(X_t, Y_t\right)$ with zero means, unit standard deviations, and correlation 0.9.
```{r}
# define a function to use Gibbs sampler to generate the chain
Gibbs.sampler<-function(N, x0){
  # N is the length of chain, x0 is the initial value
  X <- matrix(0, N, 2) # the chain, a bivariate sample
  rho <- 0.9 # correlation
  mu1<-mu2<-0
  sigma1<-sigma2<-1
  s1 <- sqrt(1-rho^2)*sigma1
  s2 <- sqrt(1-rho^2)*sigma2
  X[1, ] <- x0 #initialize
  for (i in 2:N) {
  x2 <- X[i-1, 2]
  m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
  X[i, 1] <- rnorm(1, m1, s1)
  x1 <- X[i, 1]
  m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
  X[i, 2] <- rnorm(1, m2, s2)
  }
  return(X)
  }
```

Then, I implement a Gibbs sampler to generate the chain.
```{r}
N <- 5000 # length of chain
burn <- 1000 # burn-in length
x0<-rep(0,2) # initialize
set.seed(65)
X<-Gibbs.sampler(N,x0) # generate the chain
x <- X[(burn + 1):N, ]
cat('Means: ',round(colMeans(x),3))
cat('Standard errors: ',round(apply(x,2,sd),3))
cat('Correlation coefficients: ', round(cor(x[,1],x[,2]),3))
# plot the generated sample
plot(x[,1],type='l',col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(x[,2],col=2,lwd=2)
legend('bottomright',c('X','Y'),col=1:2,lwd=2)
```

Then, I fit a simple linear regression model $Y=\beta_0+\beta_1 X$ to the sample and check the residuals of the model for normality and constant variance.
```{r}
L<-lm(x[,1]~x[,2])
summary(L)
```
As is shown above, the regression model is approximately $Y=0.9X$, which matches the parameters of the target distribution well.

Finally, I use the Gelman-Rubin method to monitor convergence of the chain.
```{r}
# define a function to use the Gelman-Rubin method to monitor convergence of the chain
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance estimation
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within estimation
  v.hat <- W*(n-1)/n + (B/n)     #upper variance estimation
  r.hat <- v.hat / W             #estimation
  return(r.hat)
}

N <- 15000 # length of chains
k <- 4    # number of chains to generate
b <- 1000       # burn-in length
# choose overdispersed initial values
x0 <- matrix(c(0,0,-0.1,0.1,0.2,-0.2,0.1,-0.2),4,2,byrow = TRUE)
set.seed(262)
# generate the chains
X <-Y<-matrix(0, nrow=k, ncol=N)
for (i in 1:k){
  X[i, ] <- Gibbs.sampler(N, x0[i,])[,1]
  Y[i, ] <- Gibbs.sampler(N, x0[i,])[,2]
}
# compute diagnostic statistics
psi.X <- t(apply(X, 1, cumsum))
psi.Y <- t(apply(Y, 1, cumsum))
for (i in 1:nrow(psi.X)){
  psi.X[i,] <- psi.X[i,] / (1:ncol(psi.X))
  psi.Y[i,] <- psi.Y[i,] / (1:ncol(psi.Y))
}

# plot the sequence of R-hat statistics
rhat.X <-rhat.Y <- rep(0, N)
for (j in (b+1):N){
  rhat.X[j] <- Gelman.Rubin(psi.X[,1:j])
  rhat.Y[j] <- Gelman.Rubin(psi.Y[,1:j])
}
# compute rhat for 2-dimensional data
rhat.XY<-matrix(c(rhat.X,rhat.Y),ncol = 2)
rhat<-apply(rhat.XY, 1, max) # find the maximum of rhat.X and rhat.Y
rhat<-sqrt(rhat)  # get rhat for 2-dimensional data
plot(rhat[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

```

As is shown above, the chain converges to the target distribution according to $\hat{R}<1.2$ when $n$ is approximately 3000 (1000+2000).
```{r}
rm(list = ls())
```




--------

<center><font color=Red size=5>**This is the end of my assignment.**</font></center>

--------

# A-22030-2022-11-04

## Question{#question}

1. Consider model
$$
M=a_M+\alpha X+e_M, \\
Y=a_Y+\beta M+\gamma X+e_Y,
$$
where $e_M\sim N(0,1)$, $e_Y\sim N(0,1)$.
Use permutation to test
(1)$\alpha=0$, (2)$\beta=0$, (3)$\alpha=0$ and $\beta=0$.

Let $\gamma=1$, consider 3 cases: (1)$\alpha=0, \beta=0$, (2)$\alpha=0, \beta=1$, (3) $\alpha=1, \beta=0$.



[Jump to the Answer](#question14ans)

2. Consider model
$$
P\left(Y=1 \mid X_1, X_2, X_3\right)=\operatorname{expit}\left(a+b_1 X_1+b_2 X_2+b_3 X_3\right),
$$
where $X_1 \sim P(1)$, $X_2 \sim Exp(1)$, $X_3 \sim B(1,0.5)$.

* Write a R function to realize above features, with input $N, b_1, b_2, b_3, f_0$ and output $\alpha$.

* Use the function with $N=10^6, b_1=0, b_2=1, b_3=-1, f_0=0.1, 0.01, 0.001, 0.0001$.

* Draw the scatter plot of $f_0$ vs. $\alpha$.

[Jump to the Answer](#question15ans)



## Answer

### Question 1{#question14ans}
**Solution.**
First, I define three functions to use permutation to test
(1)$\alpha=0$, (2)$\beta=0$, (3)$\alpha=0$ and $\beta=0$.
```{r}
library(mediation)
# define a function to conduct permutation test in situation 1: alpha=0
mediation.perm.1 <- function(X, Y, M, R=100){
  a<-lm(M~X)
  b<-lm(Y~X+M)
  result0<-mediate(model.m=a, model.y=b, sims=50, treat='X', mediator = 'M')
  T0<-abs(result0$d0/sd(result0$d0.sims)) # the original statstic
  n<-length(X)
  reps <- numeric(R)
  for (i in 1:R) {
    k <- sample(1:n)
    M1<-M[k]
    Y1<-Y[k]
    result1<-mediate(model.m=lm(M1~X), model.y=lm(Y1~X+M1), sims=50,
                     treat='X', mediator = 'M1')
    reps[i]<-abs(result1$d0/sd(result1$d0.sims))
    }
  p <- mean(c(T0, reps) >= T0) # p-value of the permutation test
  return(p) # return p-value
}

# define a function to conduct permutation test in situation 2: beta=0
mediation.perm.2 <- function(X, Y, M, R=100){
  a<-lm(M~X)
  b<-lm(Y~X+M)
  result0<-mediate(model.m=a, model.y=b, sims=50, treat='X', mediator = 'M')
  T0<-abs(result0$d0/sd(result0$d0.sims)) # the original statstic
  n<-length(X)
  reps <- numeric(R)
  for (i in 1:R) {
    k <- sample(1:n)
    M1<-M[k]
    X1<-X[k]
    result1<-mediate(model.m=lm(M1~X1), model.y=lm(Y~X1+M1), sims=50,
                     treat='X1', mediator = 'M1')
    reps[i]<-abs(result1$d0/sd(result1$d0.sims))
    }
  p <- mean(c(T0, reps) >= T0) # p-value of the permutation test
  return(p) # return p-value
}

# define a function to conduct permutation test in situation 3: alpha=0, beta=0
mediation.perm.3 <- function(X, Y, M, R=100){
  a<-lm(M~X)
  b<-lm(Y~X+M)
  result0<-mediate(model.m=a, model.y=b, sims=50, treat='X', mediator = 'M')
  T0<-abs(result0$d0/sd(result0$d0.sims)) # the original statstic
  n<-length(X)
  reps <- numeric(R)
  for (i in 1:R) {
    k <- sample(1:n)
    M1<-M[k]
    result1<-mediate(model.m=lm(M1~X), model.y=lm(Y~X+M1), sims=50,
                     treat='X', mediator = 'M1')
    reps[i]<-abs(result1$d0/sd(result1$d0.sims))
    }
  p <- mean(c(T0, reps) >= T0) # p-value of the permutation test
  return(p) # return p-value
}

```

Then, I simulate and test in 3 cases: (1)$\alpha=0, \beta=0$, (2)$\alpha=0, \beta=1$, (3) $\alpha=1, \beta=0$.

(1)$\alpha=0, \beta=0$
```{r}
am<-ay<-1; gamma<-1
n<-50
alpha<-0; beta<-0
set.seed(10)
X<-rnorm(n)
M<-am + alpha * X + rnorm(n)
Y<-ay + beta * M + gamma * X + rnorm(n)
set.seed(11)
# p-value of 3 tests
c(perm.1=mediation.perm.1(X, Y, M), perm.2=mediation.perm.2(X, Y, M),
  perm.3= mediation.perm.3(X, Y, M))
```
Therefore, we should accept $H_0$ whatever test we use.

(2)$\alpha=0, \beta=1$
```{r}
alpha<-0; beta<-1
set.seed(67)
X<-rnorm(n)
M<-am + alpha * X + rnorm(n)
Y<-ay + beta * M + gamma * X + rnorm(n)
set.seed(68)
# p-value of 3 tests
c(perm.1=mediation.perm.1(X, Y, M), perm.2=mediation.perm.2(X, Y, M),
  perm.3= mediation.perm.3(X, Y, M))
```
Therefore, we should accept $H_0$ whatever test we use.

(3)$\alpha=1, \beta=0$
```{r}
alpha<-1; beta<-0
set.seed(564)
X<-rnorm(n)
M<-am + alpha * X + rnorm(n)
Y<-ay + beta * M + gamma * X + rnorm(n)
set.seed(565)
# p-value of 3 tests
c(perm.1=mediation.perm.1(X, Y, M), perm.2=mediation.perm.2(X, Y, M),
  perm.3= mediation.perm.3(X, Y, M))
```
Therefore, we should accept $H_0$ whatever test we use.

As is shown above, the relevant test in each case is more powerful.
```{r}
rm(list = ls())
```




### Question 2{#question15ans}
**Solution.**
First, I write a function to realize above features, with input $N, b_1, b_2, b_3, f_0$ and output $\alpha$.
```{r}
simulation <- function(N, b1, b2, b3, f0){
  x1 <- rpois(N,1); x2<-rexp(N,1); x3<-rbinom(N,1,0.5)
  g <- function(alpha){
    tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
    p <- 1/(1+tmp)
    mean(p) - f0
    }
  solution <- uniroot(g,c(-50,0))
  return(solution$root)
  }
```
Then, I use the function with $N=10^6, b_1=0, b_2=1, b_3=-1, f_0=0.1, 0.01, 0.001, 0.0001$ and draw the scatter plot of $f_0$ vs. $\alpha$.
```{r}
N <- 1e6; b1 <- 0; b2 <- 1; b3<--1
f0<-c(0.1,0.01,0.001,0.0001)
set.seed(324)
# use the function and get the result
alpha<-sapply(f0, FUN=simulation, N=N, b1=b1, b2=b2, b3=b3)
rbind(f0,alpha)
# draw the scatter plot
plot(log(f0),alpha,main = expression(paste(f[0],' vs. ',alpha)))
rm(list=ls())
```




--------

<center><font color=Red size=5>**This is the end of my assignment.**</font></center>

--------

# A-22030-2022-11-11

## Question{#question}

1. Class work. [Jump to the Answer](#question16ans)

2. 2.1.3 Exercise 4 (Pages 19 Advanced in R) [Jump to the Answer](#question17ans)

3. 2.1.3 Exercise 5 (Pages 19 Advanced in R) [Jump to the Answer](#question18ans)

4. 2.3.1 Exercise 1 (Pages 26 Advanced in R) [Jump to the Answer](#question19ans)

5. 2.3.1 Exercise 2 (Pages 26 Advanced in R) [Jump to the Answer](#question20ans)

6. 2.4.5 Exercise 1 (Pages 30 Advanced in R) [Jump to the Answer](#question21ans)

7. 2.4.5 Exercise 2 (Pages 30 Advanced in R) [Jump to the Answer](#question22ans)

8. 2.4.5 Exercise 3 (Pages 30 Advanced in R) [Jump to the Answer](#question23ans)



## Answer

### Class work{#question16ans}
**Solution.**

(1) Find MLE by maximizing observed data likelihood and using the EM algorithm, and then prove that they are equal.

**Maximize observed data likelihood**

Observed data likelihood
$$
\begin{aligned}
&L_o(\lambda)=\prod_{i=1}^n P_\lambda\left(u_i \leqslant X_i \leqslant v_i\right)
= \prod_{i=1}^n (e^{-\lambda u_i}-e^{-\lambda v_i}).
\end{aligned}
$$
Then, we have
$$
l_o(\lambda)=\sum_{i=1}^{n}\log (e^{-\lambda u_i}-e^{-\lambda v_i}).
$$
Take derivative of $\lambda$, we have
$$
\sum_{i=1}^n \frac{u_ie^{-\lambda u_i}-v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0,
$$
and the MLE is the solution of the above equation.

**The EM algorithm**

The complete data likelihood is
$$
L_c(\lambda)=\prod_{i=1}^n \lambda e^{-\lambda x_i} , u_i\leq x_i \leq v_i,
$$
and then
$$
l_c(\lambda)=\log\lambda-\lambda\sum_{i=1}^{n} x_i, u_i\leq x_i \leq v_i.
$$

For $u\leq x\leq v$,
$$
f(x|u,v)=\frac{f(x,u,v)}{P(u\leq X\leq v)}
=\frac{\lambda e^{-\lambda x}}{e^{-\lambda u}-e^{-\lambda v}},
$$
then
$$
E_{\lambda}(X|u,v)=\int_{u}^{v}xf(x|u,v)dx=\frac{ue^{-\lambda u}-ve^{-\lambda v}}{e^{-\lambda u}-e^{-\lambda v}}+\frac{1}{\lambda}.
$$

Write
$$
\hat{x}_i=E_{\hat{\lambda}_0}(X_i|u_i,v_i)=
\frac{u_ie^{-\hat{\lambda}_0 u_i}-v_ie^{-\hat{\lambda}_0 v_i}}{e^{-\hat{\lambda}_0 u_i}-e^{-\hat{\lambda}_0 v_i}}+\frac{1}{\hat{\lambda}_0},
$$
then we have E-step:
$$
\begin{aligned}
&E_{\hat{\lambda}_0}\left[l_c(\lambda)|u_i,v_i,i=1,\ldots,n\right]\\
=&n\log\lambda-\lambda\sum_{i=1}^{n} \hat{x}_i\\
=&n\log\lambda-\lambda\sum_{i=1}^{n} \left(
\frac{u_ie^{-\hat{\lambda}_0 u_i}-v_ie^{-\hat{\lambda}_0 v_i}}{e^{-\hat{\lambda}_0 u_i}-e^{-\hat{\lambda}_0 v_i}}+\frac{1}{\hat{\lambda}_0}\right).\\
\end{aligned}
$$
Take derivative of $\lambda$, we have M-step:
$$
\hat{\lambda}_1=\frac{n}{\sum_{i=1}^{n} \left(
\frac{u_ie^{-\hat{\lambda}_0 u_i}-v_ie^{-\hat{\lambda}_0 v_i}}{e^{-\hat{\lambda}_0 u_i}-e^{-\hat{\lambda}_0 v_i}}+\frac{1}{\hat{\lambda}_0}\right)}.
$$
Therefore, using the recurrence formula
$$
\hat{\lambda}_{k+1}=\frac{n}{\sum_{i=1}^{n} \left(
\frac{u_ie^{-\hat{\lambda}_k u_i}-v_ie^{-\hat{\lambda}_k v_i}}{e^{-\hat{\lambda}_k u_i}-e^{-\hat{\lambda}_k v_i}}+\frac{1}{\hat{\lambda}_k}\right)},
$$
we can get the EM estimate.

Now we show that the convergence of the EM algorithm.
Since $\lambda u,\lambda v$ is small, we can assume that
$$
\sum_{i=1}^{n}
\frac{u_ie^{-\hat{\lambda}_k u_i}-v_ie^{-\hat{\lambda}_k v_i}}{e^{-\hat{\lambda}_k u_i}-e^{-\hat{\lambda}_k v_i}}\leq0
$$
holds for $k$ large enough. Thus, we have
$$
\hat{\lambda}_{k+1}\geq\hat{\lambda}_k> 0.
$$
By monotone bounded theorem, we can conclude that the EM algorithm converges.

The converged $\lambda$ estimate is the solution of the equation
$$
{\lambda}=\frac{n}{\sum_{i=1}^{n} \left(
\frac{u_ie^{-\lambda u_i}-v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}+\frac{1}{\lambda}\right)},
$$
i.e.,
$$
\sum_{i=1}^n \frac{u_ie^{-\lambda u_i}-v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0,
$$
which is equivalent to the result of maximize observed data likelihood. Therefore, the result of the two algorithms are equal.

(2) Programming to implement the two algorithms.

```{r}
# create a function to use maximize observed data likelihood to get MLE of lambda
maximize_observed_data_likelihood<-function(interval){
  g <- function(lambda){
    tmp <- (interval[,1]*exp(-lambda*interval[,1])-interval[,2]*
              exp(-lambda*interval[,2]))/(exp(-lambda*interval[,1])
                                          -exp(-lambda*interval[,2]))
    sum(tmp)
    }
  solution <- uniroot(g,c(0,20))
  return(solution$root)
}

# create a function to use the EM algorithm to get MLE of lambda
EM_algorithm<-function(interval, eplison=0.001, max_iter=100){
  lambda0<-1
  for(i in 1:max_iter) {
    tmp<-(interval[,1]*exp(-lambda0*interval[,1])-interval[,2]*
              exp(-lambda0*interval[,2]))/(exp(-lambda0*interval[,1])-
                                            exp(-lambda0*interval[,2]))+1/lambda0
    lambda<-dim(uv)[1]/sum(tmp)
    res<-abs(lambda-lambda0)
    if(res<=eplison) break
    lambda0<-lambda
  }
  return(lambda)
}
```


```{r}
uv<-rbind(c(11,12),c(8,9),c(27,28),c(13,14),c(16,17),c(0,1),
          c(23,24),c(10,11),c(24,25),c(2,3))
# use maximize observed data likelihood to get MLE of lambda
maximize_observed_data_likelihood(uv)
# use the EM algorithm to get MLE of lambda
EM_algorithm(uv)
```
As is shown above, the result of the two algorithms are similar.
```{r}
rm(list = ls())
```




### 2.1.3 Exercise 4{#question17ans}
**Problem.**
Why do you need to use unlist() to convert a list to an atomic vector? Why doesn’t as.vector() work?


**Solution.**
We take a example.
```{r}
x <- list(list(1, 2), c(3, 4))
(y1<-unlist(x))
str(y1) # a vector
(y2<-as.vector(x))
str(y2) # a list
rm(list = ls())
```
As is shown above, we can turn a list into an atomic vector with unlist(), but as.vector() can't.



### 2.1.3 Exercise 5{#question18ans}
**Problem.**
Why is 1 == "1" true? Why is -1 < FALSE true? Why is "one" < 2 false?

**Solution.**
According to the help file ?`==` :

If the two arguments are atomic vectors of different types, one is coerced to the type of the other, the (decreasing) order of precedence being character, complex, numeric, integer, logical and raw.

1 == "1": 1 is converted to "1", so actually we compare "1" == "1", it is true.

-1 < FALSE true: "FALSE" is coverted to 0, so actually we compare -1 < 0, it is true.

"one" < 2: 2 is converted to "2", so actually we compare "one" < "2", it is false.




### 2.3.1 Exercise 1{#question19ans}
**Problem.**
What does dim() return when applied to a vector?

**Solution.**
```{r}
x<-c(1,2,3)
dim(x)
rm(list = ls())
```
When using dim(x), x can be a matrix, array or data frame. Therefore, dim() return NULL when applied to a vector.



### 2.3.1 Exercise 2{#question20ans}
**Problem.**
If is.matrix(x) is TRUE, what will is.array(x) return?

**Solution.**
```{r}
x<-matrix(0,2,2)
is.matrix(x)
is.array(x)
rm(list = ls())
```
Since matrix is a special array of 2 dimension, is.array(x) return TRUE if is.matrix(x) is TRUE.




### 2.4.5 Exercise 1{#question21ans}
**Problem.**
What attributes does a data frame possess?

**Solution.**
```{r}
x<-data.frame(x=c(1,2), y=c(3,4))
attributes(x)
rm(list = ls())
```
A data frame possess attributes: names, class, and row.names.




### 2.4.5 Exercise 2{#question22ans}
**Problem.**
What does as.matrix() do when applied to a data frame with columns of diﬀerent types?

**Solution.**
```{r}
x<-data.frame(x=c(1,2), y=c('three','four'))
(y<-as.matrix(x))
typeof(y)
a<-data.frame(x=c(1,2), y=c(TRUE,FALSE))
(b<-as.matrix(a))
typeof(b)
rm(list = ls())
```
According to the help file ?`as.matrix` :

The method for data frames will return a character matrix if there is only atomic columns and any non-(numeric/logical/complex) column, applying as.vector to factors and format to other non-character columns. . Otherwise, the usual coercion hierarchy (logical < integer < double < complex) will be used, e.g., all-logical data frames will be coerced to a logical matrix, mixed logical-integer will give a integer matrix, etc.



### 2.4.5 Exercise 3{#question23ans}
**Problem.**
Can you have a data frame with 0 rows? What about 0 columns?

**Solution.**
Yes, we can create a data frame with 0 rows or 0 columns:
```{r}
x<-data.frame(matrix(nrow = 0, ncol = 2))
dim(x)
y<-data.frame(matrix(nrow = 2, ncol = 0))
dim(y)
rm(list = ls())
```




--------

<center><font color=Red size=5>**This is the end of my assignment.**</font></center>

--------

# A-22030-2022-11-18

## Question{#question}

1. Exercises 2 ((page 204, Advanced R). [Jump to the Answer](#question24ans)

2. Exercises 1 (page 213, Advanced R). [Jump to the Answer](#question25ans)

3. Implement a Gibbs sampler to generate a bivariate normal chain $\left(X_t, Y_t\right)$ with zero means, unit standard deviations, and correlation $0.9$.

- Write an Rcpp function.

- Compare the corresponding generated random numbers with pure R language using the function “qqplot”.

- Compare the computation time of the two functions with the function “microbenchmark”.

[Jump to the Answer](#question26ans)

## Answer

### Exercise 2{#question24ans}
**Problem.**
The function below scales a vector so it falls in the range [0, 1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame?
```{r}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
  }
```

**Solution.**
The function "scale01" needs numeric input, so we need check whether the elements in a data frame is numeric and use it if the column is numeric.
```{r}
# the new function, which is scale01 if the data is numeric, or return the data itself
scale<-function(x) {
  if (is.numeric(x))
    scale01(x)
  else
    x
}
# examples
x<-data.frame(a=c(1,2,3),b=c(4,5,6))
data.frame(lapply(x, scale))
y<-data.frame(a=c('one','two','three'),b=c(4,5,6))
data.frame(lapply(y, scale))
rm(list=ls())
```







### Exercise 1{#question25ans}
**Problem.**
Use vapply() to:

a) Compute the standard deviation of every column in a numeric data frame.

b) Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use vapply() twice.)

**Solution.**
```{r}
set.seed(22030)
# a) Compute the standard deviation of every column in a numeric data frame.
x<-data.frame(a=rnorm(10),b=rnorm(10,0,0.5))
vapply(x, sd, numeric(1))
# b) Compute the standard deviation of every numeric column in a mixed data frame.
y<-data.frame(a=rnorm(10),b=rnorm(10,0,0.5),c=rep(c('one','two'),c(5,5)))
vapply(y[vapply(y, is.numeric, logical(1))], sd, numeric(1))
rm(list=ls())
```



### Additional exercise {#question26ans}
**Problem.**
Implement a Gibbs sampler to generate a bivariate normal chain $\left(X_t, Y_t\right)$ with zero means, unit standard deviations, and correlation $0.9$.

- Write an Rcpp function.

- Compare the corresponding generated random numbers with pure R language using the function “qqplot”.

- Compare the computation time of the two functions with the function “microbenchmark”.

**Solution.**
```{r}
library(Rcpp)
```
- Compare the corresponding generated random numbers with pure R language using the function “qqplot”.
```{r}
set.seed(22030)
gibbR<-gibbsR(1000)
gibbC<-gibbsC(1000)
# compare X and Y
qqplot(gibbR[,1],gibbR[,2],xlab = 'x',ylab = 'y',main='QQ plot of gibbR')
qqplot(gibbC[,1],gibbC[,2],xlab = 'x',ylab = 'y',main='QQ plot of gibbC')

# compare gibbR and gibbC
qqplot(gibbR[,1],gibbC[,1],xlab = 'gibbR',ylab = 'gibbC',main='QQ plot of X')
qqplot(gibbR[,2],gibbC[,2],xlab = 'gibbR',ylab = 'gibbC',main='QQ plot of Y')
```

- Compare the computation time of the two functions with the function “microbenchmark”.
```{r}
library(microbenchmark)
ts <- microbenchmark(gibbR=gibbsR(100),
                         gibbC=gibbsC(100))
summary(ts)[,c(1,3,5,6)]
rm(list=ls())
```
As is shown above, the result of the Rcpp function is much faster than the R
function.



--------

<center><font color=Red size=5>**This is the end of my assignment.**</font></center>

--------





